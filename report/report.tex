\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{images/}}
\usepackage[cmex10]{amsmath}
\usepackage{algorithmic}
%\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{stfloats}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Automatic Mobile Video Director}

\author{
	\IEEEauthorblockN{Alexander Egurnov}
	\IEEEauthorblockA{University of Mannheim\\
		aegurnov@mail.uni-mannheim.de}
\and
	\IEEEauthorblockN{Thilo Weigold}
	\IEEEauthorblockA{University of Mannheim\\
		tweigold@mail.uni-mannheim.de}
\and
	\IEEEauthorblockN{Jon Pettersen}
	\IEEEauthorblockA{University of Oslo\\
		jonup@student.matnat.uio.no}
\and
	\IEEEauthorblockN{Alf-André Walla}
	\IEEEauthorblockA{University of Oslo\\
		alfandrw@ifi.uio.no}
}

% This a recommended way to give more than 3 authors but it looks ugly.
% Default author block above still fits in one line.
%\author{
	%\IEEEauthorblockN{
		%Alexander Egurnov\IEEEauthorrefmark{1},
		%Thilo Weigold\IEEEauthorrefmark{2},
		%Jon Pettersen\IEEEauthorrefmark{3}, 
		%Alf-André Walla\IEEEauthorrefmark{4}
	%}
	%\IEEEauthorblockA{
		%\IEEEauthorrefmark{1}University of Mannheim\\
		%aegurnov@mail.uni-mannheim.de
	%}
	%\IEEEauthorblockA{
		%\IEEEauthorrefmark{2}University of Mannheim\\
		%tweigold@mail.uni-mannheim.de
	%}
	%\IEEEauthorblockA{
		%\IEEEauthorrefmark{3}University of Oslo\\
		%jonup@student.matnat.uio.no
	%}
	%\IEEEauthorblockA{
		%\IEEEauthorrefmark{4}University of Oslo\\
		%alfandrw@ifi.uio.no
	%}
%}

\maketitle

\begin{abstract}
Advancement of data transmission technologies and availability of high quality built-in cameras 
has made sharing a video of an event someone has attended a mainstream trend.
However single amateur video rarely offers full or adequate coverage of an event.
This is where collaborative video creation techniques comes into play, which brings up new topics for exploration.
Among hundreds of videos shot at the same place at roughly the same time only 
a dozen might be worth uploading due to poor quality of most of them and limited bandwidth on the venue.
In this paper we implement an Automatic Mobile Video Director system which addresses the issue of 
selectively uploading the best videos in order to create the fullest and highest quality overview of the event.
The system consists of a client application, which shoots a video and informs the server of its' quality 
by the means of sensor metadata collected alongside shooting,
and server software, which leverages video metadata to make a decision about video uploads in near real-time.

\end{abstract}

\section{Introduction}

In the recent years the Internet has changed a lot. 
It is not about retrieving simple HTML pages from websites anymore, 
but rather sharing news, videos and pictures from all over the world.
With the advent of the ``Web 2.0'' user-generated content has become a crucial part of the Internet we know nowadays.
Social interaction on the internet is a huge aspect of everyday life for people.
Well-known platforms like YouTube, Twitch, Facebook, Twitter, Instagram, Flickr and many other
make it easy for everyone to share their creativity and thoughts in a variety of ways.

Public interest in capturing and sharing videos from public events has increased 
with the introduction of mobile devices with high resolution cameras and wireless internet connection. \cite{duggan_photo_2013}

A considerable contribution to changing the web is done by changing how people typically access it.
In the past the main role of mobile phones was one-to-one communication. 
But as the technology evolved, such devices gradually assumed functions of fully capable handheld computers.
As all the necessary operations to shoot, process and publish a complex multimedia file moved to a single device, 
the amount of user-generated content experienced an explosive growth.
Due to more precise sensors, integrated high-resolution cameras and, of course, faster mobile networking technologies, 
it has never been as simple to share content from any place and at any time.
The ubiquitous use of mobile phones leads to completely new challenges and opportunities for web services 
that use user data in order to create dynamic content.

There are so many different events happening at the same time, 
that it is easy imagine an interesting public event like a political speech, 
a concert or a sports event which is not covered by professional journalists, photo and video operators.
Fortunately, it has become quite common for spectators to capture parts of ongoing events on their portable gadgets' cameras.
Such videos are rarely organized in any reasonable manner, except for user-provided descriptions and hashtags, 
which are often abused and give little idea of the content's quality.

The potential being wasted here is tremendous. 
One obvious application for these videos is providing a near real-time media coverage of the event for internet viewers.
However, if every video is uploaded right from the venue, an excessively high bandwidth will be consumed.
But due to a vast variety of portable devices and dispersion of filming abilities of their owners,
to create of a high quality mash up of an event some elaborate selection mechanism is needed.

In this paper we attempt to build a system, called Automatic Mobile Video Director, capable of dealing with this kind of situations.
We henceforth focus on implementing interactions between multiple clients and one central server, the automatic video director,
keeping in mind bandwidth and computational complexity problems, which were mentioned above. 
As a mean of limiting the amount of data transmitted by each mobile device during the event we only require 
that some textual metadata, which is based on sensor data as well as general video parameters, is sent to the server.
The server then makes a decisions whether each particular video is worth uploading directly from the venue
and dispatches commands to clients to start uploading.
We give only a brief overview and a very basic implementation of sensor data processing as this area of research 
is being studied by another group. The same applies to the selection criteria of the director algorithm.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.

% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.

% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}

% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% However, Computer Society journals sometimes do use bottom floats - bear
% this in mind when choosing appropriate optional arguments for the
% figure/table environments.

\section{Related work}

One possible approach is discussed in \cite{engstrom_mobile_2012}.
It proposes a way of making videos where five people are able to set up a connection between their mobile devices,
assign roles to each device, one of them being assigned the role of director and the others are cameramen,
and capture an event from different perspectives. Video selection is performed manually by the designated person.

Another paper \cite{shrestha_automatic_2010} focuses on devising an algorithm which would create 
a ``mashup'' of several video and audio streams automatically.
More attention is paid to satisfying a formal model of a perfect video, 
and selection of video streams is solved as an optimization problem.
However their approach requires all video files to be present when the algorithm works, 
as it first needs to synchronize them and run image analysis.

In case of live streaming the former approach would require an overwhelming amount of data to be transmitted.
The paper \cite{seshadri_demand_2014} proposes a method of reducing data traffic significantly.
They achieve it through the use of metadata captured by all kinds of sensors which are present on modern handheld devices.
The metadata helps to evaluate quality of videos prior to uploading them from client devices on the event venue to the server.
This way only the best videos actually get uploaded and consume traffic.
As it is pointed out in the paper, this is a huge issue for some extremely popular events like USA's SuperBowl \cite{Erman:2013}.

Systems like the ones discussed above can be used to generate a summary of an event by fusing parts of different video clips together.
However, to discover exactly when a video is shot also represents a challenge. 
Failing to find the exact time a video is shot may lead to videos overlapping or failing to cover the entire event 
by leaving gaps in the video timeline \cite{shrestha_automatic_2010}. 
An approach leveraged in \cite{jain_focus:_2013} is to use GPS to extract timestamps. 
Another way to addresses this issue is by using the audio in the video files 
to generate a unified timeline of the event \cite{shrestha_automatic_2010}. 

A different issue is identifying a region of interest or point of interest.
These terms describe areas of the event that are more interesting than others. 
We probably will find a video of a concert more interesting if the camera is pointing towards the stage than one pointing somewhere else.
Several solutions have been proposed for solving this problem.
One of them is to go through frames of the video to see if they resemble the main attraction of the event, 
i.e. a stage on a concert, or the singer on that stage. 
However the reliance on the sensors in the mobile devices for discovering such regions seems to be a more preferable method.
\cite{cricri_sensor-based_2012} uses the built-in compass found on mobile devices to make sure 
that the camera is pointing in the right direction.
Their system also uses compass to detect when camera changes direction. 
An assumption is made that if operator suddenly turns his or her camera in some direction,
there is a chance that something interesting is happening there.

When it comes to detecting what is pictured in a video, one might also want to identify the activity which was filmed.
This can be used to relieve managers of such systems from the trouble of marking each video with appropriate tags. 
Authors of \cite{cricri_sport_2014} have created a system that is capable of detecting which of several kinds of sport is being filmed.

Another issue is when to start recording and how to define what is an event.
In the paper \cite{bao_movi:_2010} a system is described which can detect when a social event is taking place, leveraging
mobile devices sensors and communication between nearby devices.
The devices then start recording and ultimately a ``highlights'' video coverage is created.
Among other things, authors use the sound of laughter to trigger event detection.

\section{Methodology}

\subsection{System Overview}
In this section we give a brief overview of the whole system.
Detailed discussions of our implementation will follow in the subsections afterwards. 
To implement our idea of an Automatic Mobile Video Director it was necessary to find the right structural organization for our system.
Given requirements and logical distribution of tasks leads us to the decision 
to implement our service in a conventional client-server architecture.
Figure~\ref{fig:gen_arch} shows our general system architecture, consisting of multiple clients and one central server.

This architectural set up entails some important advantages.
Client-Server design gives us opportunity to strictly separate tasks and work between clients and server.
Compared to other possible architectures, like peer to peer, we are able to centralize all resources
and important computation in one place. 
Resources, like battery and computational effort can be saved on the client side, 
while computationally intensive tasks are carried out on the server side. 
In peer to peer networks clients would have to communicate with each other, 
which would produce big communication overhead to weight of important resources.
However, these resources are limited and needed for recording videos and sending them upon request to our central Video Director.
Therefore we believe that the client-server architecture best matches our requirements.

The server set up uses local file system for video storage, a MySQL database for storing and retrieving meta data persistently and a RESTful interface to handle HTTP requests and responses.
Additional core function is the director algorithm which is used to make decisions about which video clients should upload.
Nginx web server is used to stream resulting video from our web server to viewers.
This feature is not fully implemented yet and left for future work and improvements.
Detailed descriptions of our server and client implementation are discussed in the following sections.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.45\textwidth]{sys_arch.png}
	%\includegraphics[width=0.45\textwidth]{overview.jpg}
	\caption{General system overview}
	\label{fig:gen_arch}
\end{figure}

\subsection{Client application}

The client application is designed in four main logical components: 1. HTTP connection, 2. camera and sensors, 3. the persistent database and 4. user interface.

\subsubsection{HTTP connection}
As a communication protocol between client and server we decided to use HTTP.

Two major options for HTTP clients are Apache HTTP Client and \texttt{HttpURLConnection}.
Since it is recommended to use \texttt{HttpURLConnection} for Android Gingerbread and later \cite{jesse_wilson_androids},
we decided to go with it.
It is lightweight, simple to handle as well as applicable for all purposes we are aiming to do.
When a \texttt{HttpUrlConnection} is obtained, the application's main thread is used and blocked as long as the connection is open. \cite{gilles_debunne_androids}.

Therefore unpredictable delays might result in poor usability experiences of the user interface.
In order to ensure a smooth work flow of the application, we have to make use of specific thread techniques.

First it is necessary to prevent network operations blocking the application's user interface.
Doing so all network operations need to be run as an instance
of the specified \texttt{HttpAsyncTask} class, which extends Android's \texttt{AsyncTask} class, recommended by Google to use, in order to fire new asynchronous Tasks from the UI thread \cite{gilles_debunne_androids}.

Each instance of our \texttt{HttpAsyncTask} class, defines a new \texttt{HttpUrlConnection}.

Thus request type, request url, meta data and a callback function have to be indicated as parameters to create this instance.
Afterwards a new \texttt{HttpUrlConnection} can be fired within the \textsl{doInBackground(String...urls)} function of the \texttt{HttpAsynTask} class. Consequently all \texttt{HttpUrlConnection} tasks,
sending \texttt{OutputStream}s and reading \texttt{InputStream}s are done independently of the UI thread.
Since we need to make special requests to the server, this class serves as an interface to all network operations.
Client session identification on the server is implemented through cookie mechanism. 
On order to support it, a custom cookie storage was implemented based on a freely available example \cite{so_java_cookies}.

As well the application requires an automatic upload of selected videos, even when users leave the application.
Such feature can be realized in Android by implementing a service class, which handles background computations in the main thread. 
A second thread within this service class is used to request potential selected video ids via HTTP GET every 10 seconds,
in order to upload video files via HTTP PUT if necessary.
By doing so we make sure, that all network operations run independently and separated of our main thread.

\subsubsection{Camera \& Sensors}
Further core elements of the application are camera and sensors, used for exclusively capturing videos and sensor data simultaneously.
For video recording we do not use the phone's default camera application,
rather writing our own camera activity, which can be suited to our requirements and implemented in the \texttt{CameraActivity} class.
Using Android's \texttt{MediaRecorder} class we are able to handle access to the phone's integrated camera
as well as the recording and the storage of video files.

Figure~\ref{fig:mediarecorder} \cite{mediarec_doc} shows the supposed behavior of the \texttt{MediaRecoder} in its different states. 
The function \texttt{CameraActivity}.\textsl{prepareMediaRecorder()} follows the supposed main strand,
by setting the Video and Audio Input Sources, defining the Output format (MP4) and setting the preview surface for displaying the camera stream.
After preparing the \texttt{MediaRecorder} it is ready for starting the recording, which is triggered in the \textsl{recordListener()}.

Simultaneously to video capturing, sensor data is tracked as well.
In order to couple sensor and video capture we make use of the observer pattern.
Sensor detection classes \texttt{ShakeDetection} and \texttt{TiltDetetcion} are implemented as Observables,
while the \texttt{CameraActivity} class registers as a single observer. 
When sensor data changes, the \texttt{CameraActivity} is notified by the observables.
All in all we have decided not to focus on sensor detection and retrieving.
A lot of research has been already done in those parts. Due to the shortage of available time we only retrieve 
some basic accelerometer data, in order to determine the amount of shaking and weight of tilt, 
represented by simple integer values.
The sensor data is part of the meta data which is sent to the server for directing the video upload process.
Hereby the amount of shaking is a counter which is increased by one, when ever shaking is detected.
We assume to be in a shaking motion, when at least two of three accelerometer values have changed related
to their previous values by a specific threshold.
The threshold value is initialized as a float constant with the value 0.7f. This value is exclusively based on experimenting and testing.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.45\textwidth]{mediarecorder.jpg}
	\caption{MediaRecorder: State Diagram}
	\label{fig:mediarecorder}
\end{figure}

\subsubsection{SQLite database}
In order to keep track and coupling of stored video files and obtained meta data, we make use of a persistent database,
implemented with the lightweight SQLite library.
The local database set up would not have been entirely necessary. 
But due to simplicity and considerations for future extensions, we decided to have all data in persistence.
Each meta data entry has its unique \textsl{id} and each stored video file has its unique \textsl{filename}.
Therefore we use the database column \textsl{filename} as a unambiguous reference to a specific video file.
When the client gets notified by the server to upload a video, it is necessary to check the filename in the within 
the table entries, selecting the correct file to be uploaded.

\subsection{Server application}

The server was created using Java 8, Maven, Spark (for Java 8), jetty and Gson. Maven allows simplified project
management and deployment, and spark allows creating RESTful service using Java 8's lambdas, which
simplifies client-server interactions. Gson was used to communicate with clients as well as serialize
and deserialize SQL results. jetty is the web server which processes all incoming requests and outgoing responses.

The server application serves content using a RESTful API provided by Spark. This is done through using Sparks
lambda functions to define REST API actions and then Gson to convert to and from JSON trees.

Everything starts in the main class Main, where all the REST actions are defined and implemented. From here,
the main class communicates with an instance of the Director class which acts as the manager. The main
class is also responsible for sending and receiving videos through processing HTTP input and output streams
as well as maintaining client sessions.

Client sessions are used to manage the video upload system, where clients get assigned videos they have uploaded
metadata for, and if a video gets selected they are allowed to upload the selected videos to the server.
Sessions are part of the Spark library, and allows us to uniquely identify short-term client connections. Using
sessions means we don't have to have a registration system in place to uniquely identify clients, which in itself
has both good and bad sides to it. The system is designed with this in mind, but does not have any mechanisms in
place to detect bad behavior from clients, such as forged metadata.

The \texttt{Director} class manages communication from the web-frontend with the database backend. It has an internal list
of Client instances which are existing client sessions, and it also rates videos by metadata before they are
stored in the database.

The \texttt{DatabaseHandler} class manages communication with a database primarily using JSON as both input and output.
This is done through functions which serialize and deserializes data. All the timestamp-based functionality needed
for selecting videos sequentially are done through database queries using mySQL unix\_timestamp() conversions of
video and event timestamps.

Two versions of the server-side database have been created for this project. The first was developed by Jon while the 
second was developed by Alf and Alexander. The first had one table for video with variables for event id, client id 
and video id and used these as primary key. In addition all metadata was stored here. It also had a table, event that 
consisted of the values event id and event name. There was a foreign key between event ids in these tables such that there 
had to be an event id in event for there to be inserted an event id in video.
The current database has one table for video with variable id and metadata. In addition it has a table for events and a table 
called event_video. Events stores information about an event and event_video acts as the connection between event and video.


The \texttt{VideoRating} class provides static functionality to rank videos by certain criteria. This is done by providing
a set of data that depends on the video duration. In the current implementation, you input video metadata and the
rating system will transform these values, which are defined from 0 to infinity, down to [0, 1] using a sigmoid function,
then account for the duration of the video. This is done because shorter videos should get heavier penalties for
having the same amount or more shaking and tilting as a longer duration video. The function is applied separately to
shaking and tilting, and afterwards they are combined into a score using a kernel with weights. The final score is
then multiplied by 100, so that the final range is [0, 100]. This process happens before video metadata is stored
in the database, as the video itself never changes, and helps the video upload selection process later on.

A final part of server software as from video lifeline's point of view is an event video stream from web server.
Unfortunately, this work although being not especially hard, was very different from all the other tasks and
required some web programming which we decided to sacrifice in favor of other parts which are more essential to the problem statement.
Moreover, another group already works on a web based framework designed to show users on-demand video streams.
However, we did not completely neglect this part of server functionality. 
We use a nginx \cite{nginx} web server with a Real Time Messaging Protocol (RTMP) streaming module \cite{nginx-rtmp-module}.
Given video ID, one can construct a link which gives access to RTMP streaming of required video.
RTMP protocol has a number of advantages over classical pseudo-streaming of mp4 files, the most outstanding of them being
free navigation of the content without the need to download the whole video.
The module we use also supports HTTP Live Streaming technology \cite{apple-hls} which is widely supported by Apple Inc. devices.
Both streaming protocols are supported by video.js \cite{video-js} HTML5 video player and can be embedded on a web page.

\subsection{Client-server interaction}

All client server interaction are carried out using HTTP requests to certain server locations.
Server provides a kind of API which allows client application to upload metadata, query selection results and upload videos.
We decided to use HTTP because it is textual, which means easy to debug, 
wide-spread and well established protocol with a lot of tool already available. 
Another advantage of this protocol is that it does not require to maintain a persistent connection between the two ends.
This important feature allows to prolong battery life and diminish traffic waste.

\subsection{Protocol}

Our Automatic Mobile Video Director server implementation provides 
a generalized interface to applications which wish to interact with it. 
It is implemented through RESTful HTTP requests, using certain server locations as actions.

\begin{description}
	\item[GET /events]\hfill\\
		Lists all events (including videos) in~JSON.
		
	\item[GET /event/\textit{id}]\hfill\\
		Returns Event (including videos) in~JSON.
				
	\item[POST /event/new]\hfill\\
		Create new event from JSON.
		Expects request body to be a JSON string containing attribute~\textit{name}.
		
	\item[POST /event/\textit{id}]\hfill\\
		Upload JSON metadata about a video for Event with given \textit{id}.
		
	\item[PUT /video/\textit{video\_id}]\hfill\\
		Upload video~\textit{video\_id} from Event~\textit{id}.
		Expects request body to be a file stream containing a full video file.
		
	\item[GET /video/\textit{video\_id}]\hfill\\
		Retrieve video~\textit{video\_id} from Event~\textit{id}.
		
	\item[GET /selected]\hfill\\
		Retrieve a list of selected but not yet uploaded videos in JSON.	
		
\end{description}

A typical data exchange looks as follows:
Every 10 seconds client queries server about new selected videos with GET on /selected.
If clients happens to have no assigned \textsl{ClientID} on the server or the cookie the client 
sent is not recognized, a new \textsl{ClientID} is issued and a new cookie is set.
After a client finishes shooting a video, its' matadata is POSTed to currents events' thread /event/\textit{id}
and a server-side video ID is returned in response.
If the video later appears in response to one of the GET /selected queries, an upload process is initiated.
For illustration see Figure~\ref{fig:protocol}.
Event management API calls are not currently used in client application.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.45\textwidth]{protocol.png}
	\caption{Client-server data exchange}
	\label{fig:protocol}
\end{figure}

\subsubsection{Meta data format and exchange}
The exchange of meta data is an important part of our client - server communication.
Therefore it is necessary to define a data interchange format, which fulfills the requirements to describe the meta data properly.
By implementing a RESTful Web Service, two formats are particularly interesting to consider: JSON and XML.
Both formats have their advantages and drawbacks, which are explained in context for our decision making in the following.
%TODO INSERT PAPER Nurzhan Nurseitov + BOOK RICHARSDON & RUBY

JSON is a lightweight and a human-readable data-interchange format. Its structural Strings resembles simple data structures in general. 
It is easy to use and parse for computers, which leads to an advantage to save computational power on client and server side. 
Serialization and deserialization between JSON Strings and Objects can be done efficiently, with less effort in comparison to XML \cite{Nurseitov:2009}.
According to Richardson and Ruby XML is mainly used when it is necessary to describe data structures, which fit into the document paradigm \cite{Richardson:2007}. 
Therefore exchanging meta data in our case does not come with the need to use complex and profound structures like XML. 
Within the HTTP POST REQUEST and HTTP POST RESPONSE, as well as the HTTP GET RESPONSE  of our application, 
the request-/response body can be described as a simple JSON string. In the following all the meta data exchange of our HTTP REQUESTS and 
RESPONSES are shown in the used JSON format:

\begin{description}
	\item[POST /event/\textit{event\_id}]\hfill\\	
				REQUEST: \textit{\{ name:””, finish\_time:””, duration:””, width:””, height:””, shaking:””, tilt:”” \}}\hfill \\
				RESPONSE: \textit{\{ id:””, name:”” \}}

	\item[GET /selected]\hfill\\		
				REQUEST: \textit{\{\}}\hfill \\
				RESPONSE:\textit{\{ id:””, event\_id:””, finish\_ts:””, status:”” \}}

\end{description}

Due to its simple and lightweight character we are able to reduce data traffic between clients and server. 
XML comes with big chunks of data in raw text format, which makes the size overhead in comparison to JSON crucial.
The data traffic amount might seem not differ much at first, but considering that a plurality of clients request GET/selected every 10 seconds and upload videos via the mobile network, client costs and bandwidth problems are definitely minimized using JSON. Therefore we decide to use JSON as our preferred interchange format language.

The meta data consists of following attributes:

\begin{description}
	\item[id] 
		Server-side unique identification of the video. 
		On Client side named as server\_id, while id the client-side unique identifier represents.

	\item[name]
		File name in client's local file system. On client side called filename.
		
	\item[finish\_time]\hfill\\
		Video creation time. On client side named as timestamp. 

	\item[duration]\hfill\\
		Video duration.
		
	\item[width]\hfill\\
		Video frame width in pixels.

	\item[height]\hfill\\
		Video frame height in pixels.		
	
	\item[shaking]\hfill\\
		Amount of shaking detected by sensors.

	\item[tilt]\hfill\\
		The amount of tilt holding of the camera.
	
	\item[status]\hfill\\
		Video status. Indicates video life cycle phase.

		
\end{description}

\section{Video Director Algorithm}

\subsection{Video life cycle}

Video metadata is stored in the database upon receiving it from a client. A temporary video instance
is then created on the server to manage interactions with a short-lived client session. Once that session
is over, the client will no longer be able to upload his videos, unless he uploads the same metadata again.
The server would likely not accept these new videos as a great deal of time would have passed. Any live stream
would be running far ahead of the time these videos were shot. After the client has uploaded metadata, the
videos are store in the database with the status flag set to 0 (METADATA). To be able to upload a video,
the client must connect to the server, and issue a HTTP GET call on /selected, where the server will make
a decision on which videos of his that will be eligible for upload. If a video is selected, identifying
information is returned as a JSON string, and the client can then issue HTTP POST with path /video/[id].
Once the video has been received, its status is set to 1 (RECEIVED). A video can further be promoted to
status 2 (PUBLISHED), in which it could be visible for, say, an event stream.

Due to many videos not being actively uploaded, and older videos still remaining, a pruning mechanism could
be implemented to clean out older videos and metadata.

\subsection{Selection algorithm}

When selecting a video for upload, given a client, the server will consider all pending videos that client,
then for each event of a video (should be just one) check if the client has any pending videos that
are top candidates for that events stream. This is done by asking for the subset of videos which start after
the event timestamp, and then sorting by video rank. This ensures that the video closest to the events
timestamp gets selected, so that its possible to maintain a stream of videos sequentially which are determined
to be of good quality. It also unfortunately means that there can be gaps between videos, and that with not
enough videos low quality will be selected. After receiving a selected video, the server immediately sets the
new event timestamp, so even if a new video appears that is of much higher quality, or is more directly
sequential to the last one, the server will end up using the ``guaranteed'' video it just received. This could
be alleviated somewhat by having a potential stream running with a bigger delay from the main event, and then
doing more intelligent rebuilding of the stream.

\section{Evaluation}
How good/bad it is.

\subsection{Data Traffic} 

1. Measure data consumption
2. Find data from other video streaming service
3. Comparison

\subsection{Selection criteria}

Describe the algorithm we use now
1. We assign some rating based on metadata
2. We select video based on where the event is at, based on video timestamps
etc.

\section{Future Work}

Our implementation is far from perfect, especially compared to similar existing systems.
This is due to time constraints limiting some of the features we have, and could have added to our implementation.
Among the things which could be improved are more intelligent identification of timelines belonging to events,
flexible ways of identifying regions of interest, also without compromising user privacy, real-time streaming from mobile devices to server 
and from server to end users with minimum delay, a web portal to navigate covered events and more.
In this section we will go through these points, look at the problems we encounter with our solution 
and suggest better implementation alternatives where available.

Timestamps represent a weakness in our system. They are generated based on the clock in the mobile
device which is not necessarily accurate or synchronized to server time or even timezone. They can also
be falsified, since the server has no way to confirm a given video was taken at a specific moment in time.
This creates uncertainty about the actual time a video was captured as it relies on us trusting the devices
all have the exact same time. This is practically impossible as the system is intended for use by general public. 
There are two ways presented in the sources for this paper for solving this. 
\cite{shrestha_automatic_2010} presents a system that uses the audio of a video file to pinpoint the duration of an event it covers.
It does this by comparing audio to other video files looking for what they call “audio-fingerprints”. 
According to them, it is a robust method that calculates the time offset of a video in about 11.6 ms. 
Another way is just mentioned in \cite{ jain_focus:_2013} and uses the GPS device located on a mobile device
to generate what they call “precise GPS-derived timestamps”.

Region of interest is a very interesting topic in this field as it focuses on making sure 
that the right content is captured during an event.
The fact that we do not have it implemented has a few implications to the practical use of our system.
First of all, we cannot be sure what is shown on the video. 
We do not check where the mobile device is when it films so the person filming may not be at a listed event at all.
We can solve this by checking the users GPS when they try to connect to our server, or when they deliver a video file.
When creating an event we would have to mark up the coordinates where the event is taking place 
and only accept videos delivered from within the designated area.
Even if we make sure the person is at the event we cannot be sure he or she is pointing the camera in the right direction.
This problem can be solved by checking the built-in compass in the mobile device 
and only accepting videos that are shot in the right direction, as proposed in \cite{cricri_sensor-based_2012}.
So if we want to film a concert we only accept files from clients that are at the event and are pointing their camera at the stage. 
Identifying where the videos are shot and what they show is also impacted by issues of privacy and commercial interests. 
With our system we can shoot videos anywhere, at any time, but this can come in conflict with legal rights 
if we use the application to film a person without their consent.
We would also come in conflict with TV-channels, sitting on the rights for covering a sports event, 
if we use our system to provide a free alternative to theirs, without their permission. 
Making sure the system can properly identify the regions of interest is therefore very important.

When we talk about multimedia system, one of the most important results is delivered content. 
While we do implement some limited streaming capabilities, a proper web portal, which allows to choose an event 
and see the stream right from a browser, would be a logical final to video's path through out system.
It is not hard to implement, but it needs time and some basic web programming knowledge which is quite different from our other tasks.

\section{Conclusion}
In this paper we present an application that is able to create a video summary of an event using only metadata collected from mobile devices
filming at that event. This system includes an application deployable on devices running android and a website capable of playing the summaries as streams. 
While the system lacks some key features, like identification of region of interest and an advanced selection algorithm for which videos we choose for a 
summarization, this was not the focus of this project. The focus was getting the infrastructure for such a system in place and we have done just that. 

% TODO
WRITE ME:
EXAMPLE:
We built a system with video selection algorithm using metadata from clients and so on.
What did we achieve?

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
\bibliography{report}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}

%\newpage

\begin{table*}[t]
	\centering
	\renewcommand{\arraystretch}{1.5}
	\caption{Task distribution}
	\label{tab:task_distr}
	\begin{tabular}{lllr}
		\toprule
		Part & Task & Subtask & Responsible \\
		\midrule
		Design
			& Server application & & Alf-André Walla \\
			& Client application & & Thilo Weigold \\
			& Client-server interactions & & Alexander Egurnov, Alf-André Walla \\
		\midrule
		Android application
			& Video Capture & Implementation & Thilo Weigold \\
			&       & Autofocus & Alexander Egurnov \\
			& MediaManager & & Thilo Weigold \\
			& Sensor data collection & & Thilo Weigold \\
			& Metadata class & & Thilo Weigold \\
			& Http Client & Post Method & Thilo Weigold \\
			&       & Cookies, Callbacks & Alexander Egurnov \\
			&       & Get \& Upload methods & Alexander Egurnov \\
			& Background upload service & & Thilo Weigold \\
			& SQLite database connection & & Thilo Weigold \\
			& Preferences & & Alexander Egurnov \\
			& GUI & & Thilo Weigold, Alexander Egurnov \\
			& Client-server data exchange & & Alexander Egurnov \\
		\midrule
		Server application 
			& RESTful Server application & Client authorization & Alf-André Walla \\
			&       & Request processing & Alf-André Walla \\
			&       & Video and Event logic & Alf-André Walla \\
			& MySQL database connection & & Jon Pettersen \\
			& Video Director & & Alf-André Walla \\
			& Client-server data exchange debug & & Alexander Egurnov \\
			& Video upload & & Alf-André Walla, Alexander Egurnov \\
		\midrule
		Evaluation
			& Data usage & & \\
		\midrule
		Web Server
			& MySQL Administration & & Alexander Egurnov \\
			& Nginx setup for streaming video & & Alexander Egurnov \\
		\midrule
		Documentation 
			& Basic template formatting & & Alexander Egurnov \\
			& General editing & & Alexander Egurnov \\
			& Images and other data & & Thilo Weigold, Alexander Egurnov \\
			& Introduction & & Thilo Weigold, Alexander Egurnov \\
			& Related Work & & Jon Pettersen, Alexander Egurnov \\
			& Methodology & System Overview & Thilo Weigold \\
			&		& Client application & Thilo Weigold \\
			&		& Server application & Alf-André Walla, Jon Pettersen \\
			&		& Client-server interactions & Alexander Egurnov \\
			& Video Director & & Alf-André Walla \\
			& Evaluation & & \\
			& Future Work & & Jon Pettersen \\
			& Conclusion & & \\
		\bottomrule
    \end{tabular}%
\end{table*}%

\vfill

\end{document}
