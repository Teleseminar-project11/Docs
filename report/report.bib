
@inproceedings{shrestha_automatic_2010,
	address = {New York, {NY}, {USA}},
	series = {{MM} '10},
	title = {Automatic Mashup Generation from Multiple-camera Concert Recordings},
	isbn = {978-1-60558-933-6},
	url = {http://doi.acm.org/10.1145/1873951.1874023},
	doi = {10.1145/1873951.1874023},
	abstract = {A large number of videos are captured and shared by the audience from musical concerts. However, such recordings are typically perceived as boring mainly because of their limited view, poor visual quality and incomplete coverage. It is our objective to enrich the viewing experience of these recordings by exploiting the abundance of content from multiple sources. In this paper, we propose a novel {\textbackslash}Virtual Director system that automatically combines the most desirable segments from different recordings resulting in a single video stream, called mashup. We start by eliciting requirements from focus groups, interviewing professional video editors and consulting film grammar literature. We design a formal model for automatic mashup generation based on maximizing the degree of fulfillment of the requirements. Various audio-visual content analysis techniques are used to determine how well the requirements are satisfied by a recording. To validate the system, we compare our mashups with two other mashups: manually created by a professional video editor and machine generated by random segment selection. The mashups are evaluated in terms of visual quality, content diversity and pleasantness by 40 subjects. The results show that our mashups and the manual mashups are perceived as comparable, while both of them are significantly higher than the random mashups in all three terms.},
	urldate = {2014-09-23},
	booktitle = {Proceedings of the International Conference on Multimedia},
	publisher = {{ACM}},
	author = {Shrestha, Prarthana and de With, Peter H.N. and Weda, Hans and Barbieri, Mauro and Aarts, Emile H.L.},
	year = {2010},
	keywords = {mashups, multiple-camera recordings, perceptual video quality, synchronization, user evaluation},
	pages = {541--550},
	file = {ACM Full Text PDF:D\:\\Mannheim\\Zotero\\storage\\CMF2EABV\\Shrestha et al. - 2010 - Automatic Mashup Generation from Multiple-camera C.pdf:application/pdf}
}

@inproceedings{cricri_sensor-based_2012,
	address = {Berlin, Heidelberg},
	series = {{MMM}'12},
	title = {Sensor-based Analysis of User Generated Video for Multi-camera Video Remixing},
	isbn = {978-3-642-27354-4},
	url = {http://dx.doi.org/10.1007/978-3-642-27355-1_25},
	doi = {10.1007/978-3-642-27355-1_25},
	abstract = {In this work we propose to exploit context sensor data for analyzing user generated videos. Firstly, we perform a low-level indexing of the recorded media with the instantaneous compass orientations of the recording device. Subsequently, we exploit the low level indexing to obtain a higher level indexing for discovering camera panning movements, classifying them, and for identifying the Region of Interest ({ROI}) of the recorded event. Thus, we extract information about the content without performing content analysis but by leveraging sensor data analysis. Furthermore, we develop an automatic remixing system that exploits the obtained high-level indexing for producing a video remix. We show that the proposed sensor-based analysis can correctly detect and classify camera panning and identify the {ROI}; in addition, we provide examples of their application to automatic video remixing.},
	urldate = {2014-09-23},
	booktitle = {Proceedings of the 18th International Conference on Advances in Multimedia Modeling},
	publisher = {Springer-Verlag},
	author = {Cricri, Francesco and Curcio, Igor D. D. and Mate, Sujeet and Dabov, Kostadin and Gabbouj, Moncef},
	year = {2012},
	keywords = {analysis, compass, indexing, multi-camera, sensor, video},
	pages = {255--265}
}

@article{seshadri_demand_2014,
	title = {On Demand Retrieval of {CrowdSourced} Mobile Video},
	volume = {Early Access Online},
	issn = {1530-437X},
	doi = {10.1109/JSEN.2014.2336292},
	abstract = {The proliferation of mobile cameras has popularized social-sharing of videos captured at events such as sports matches, art performances, and lectures. Due to bandwidth and energy constraints, it is often not efcient or desirable to upload all captured videos to a server for sharing immediately after capturing. We propose a pull-based, on-demand mobile video sharing system that allows user to share video captured at an event, with two novel components: (i) a lightweight video metadata extraction algorithm running on the smartphones that considers both temporal and spatial features, including sensor (compass) readings and point-of-interest ({POI}) of the content; and (ii) a video selection algorithm, running on the server, that responds to user queries considering both accuracy of the retrieved video and the upload cost of the smartphones. Our evaluation results show up to 4 times improvement in upload cost and 52\% improvement in subjective quality over three baseline algorithms that consider only either cost or accuracy.},
	journal = {{IEEE} Sensors Journal},
	author = {Seshadri, P. and Chan, M. and Ooi, W. and Chiam, J.},
	year = {2014},
	keywords = {Accuracy, Cameras, compass, Mobile communication, Sensors, Servers, Smart phones},
	file = {IEEE Xplore Abstract Record:D\:\\Mannheim\\Zotero\\storage\\6JCAGFMQ\\abstractKeywords.html:text/html;IEEE Xplore Full Text PDF:D\:\\Mannheim\\Zotero\\storage\\MEJR5SNT\\Seshadri et al. - 2014 - On Demand Retrieval of CrowdSourced Mobile Video.pdf:application/pdf}
}

@incollection{cricri_sensor-based_2012-1,
	series = {Lecture Notes in Computer Science},
	title = {Sensor-Based Analysis of User Generated Video for Multi-camera Video Remixing},
	copyright = {©2012 Springer-Verlag {GmbH} Berlin Heidelberg},
	isbn = {978-3-642-27354-4, 978-3-642-27355-1},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-27355-1_25},
	abstract = {In this work we propose to exploit context sensor data for analyzing user generated videos. Firstly, we perform a low-level indexing of the recorded media with the instantaneous compass orientations of the recording device. Subsequently, we exploit the low level indexing to obtain a higher level indexing for discovering camera panning movements, classifying them, and for identifying the Region of Interest ({ROI}) of the recorded event. Thus, we extract information about the content without performing content analysis but by leveraging sensor data analysis. Furthermore, we develop an automatic remixing system that exploits the obtained high-level indexing for producing a video remix. We show that the proposed sensor-based analysis can correctly detect and classify camera panning and identify the {ROI}; in addition, we provide examples of their application to automatic video remixing.},
	language = {en},
	number = {7131},
	urldate = {2014-09-23},
	booktitle = {Advances in Multimedia Modeling},
	publisher = {Springer Berlin Heidelberg},
	author = {Cricri, Francesco and Curcio, Igor D. D. and Mate, Sujeet and Dabov, Kostadin and Gabbouj, Moncef},
	editor = {Schoeffmann, Klaus and Merialdo, Bernard and Hauptmann, Alexander G. and Ngo, Chong-Wah and Andreopoulos, Yiannis and Breiteneder, Christian},
	month = jan,
	year = {2012},
	keywords = {analysis, compass, Database Management, Image Processing and Computer Vision, indexing, Information Storage and Retrieval, Information Systems Applications (incl. Internet), multi-camera, Multimedia Information Systems, Pattern Recognition, sensor, video},
	pages = {255--265},
	file = {Full Text PDF:D\:\\Mannheim\\Zotero\\storage\\A8X94T3T\\Cricri et al. - 2012 - Sensor-Based Analysis of User Generated Video for .pdf:application/pdf;Snapshot:D\:\\Mannheim\\Zotero\\storage\\2UPZ5MFA\\10.html:text/html}
}

@inproceedings{engstrom_mobile_2012,
	address = {New York, {NY}, {USA}},
	series = {{MUM} '12},
	title = {The Mobile Vision Mixer: A Mobile Network Based Live Video Broadcasting System in Your Mobile Phone},
	isbn = {978-1-4503-1815-0},
	shorttitle = {The Mobile Vision Mixer},
	url = {http://doi.acm.org/10.1145/2406367.2406390},
	doi = {10.1145/2406367.2406390},
	abstract = {Mobile broadcasting services, allowing people to stream live video from their cameraphones to viewers online, are becoming widely used as tools for user-generated content. The next generation of these services enables collaboration in teams of camera operators and a director producing an edited broadcast. This paper contributes to this research area by exploring the possibility for the director to join the camera team on location, performing mixing and broadcasting on a mobile device. The Mobile Vision Mixer prototype embodies a technical solution for connecting four camera streams and displaying them in a mixer interface for the director to select from, under the bandwidth constraints of mobile networks. Based on field trials with amateur users, we discuss technical challenges as well as advantages of enabling the director to be present on location, in visual proximity of the camera team.},
	urldate = {2014-09-23},
	booktitle = {Proceedings of the 11th International Conference on Mobile and Ubiquitous Multimedia},
	publisher = {{ACM}},
	author = {Engström, Arvid and Zoric, Goranka and Juhlin, Oskar and Toussi, Ramin},
	year = {2012},
	keywords = {bandwidth, broadcast, collaboration, live, mobile network, mobility, production, real-time mixing, video, vision mixer, webcasting},
	pages = {18:1--18:4},
	file = {ACM Full Text PDF:D\:\\Mannheim\\Zotero\\storage\\MVMEU233\\Engström et al. - 2012 - The Mobile Vision Mixer A Mobile Network Based Li.pdf:application/pdf}
}

@inproceedings{simoens_scalable_2013,
	address = {New York, {NY}, {USA}},
	series = {{MobiSys} '13},
	title = {Scalable Crowd-sourcing of Video from Mobile Devices},
	isbn = {978-1-4503-1672-9},
	url = {http://doi.acm.org/10.1145/2462456.2464440},
	doi = {10.1145/2462456.2464440},
	abstract = {We propose a scalable Internet system for continuous collection of crowd-sourced video from devices such as Google Glass. Our hybrid cloud architecture, {GigaSight}, is effectively a Content Delivery Network ({CDN}) in reverse. It achieves scalability by decentralizing the collection infrastructure using cloudlets based on virtual machines{\textasciitilde}({VMs}). Based on time, location, and content, privacy sensitive information is automatically removed from the video. This process, which we refer to as denaturing, is executed in a user-specific {VM} on the cloudlet. Users can perform content-based searches on the total catalog of denatured videos. Our experiments reveal the bottlenecks for video upload, denaturing, indexing, and content-based search. They also provide insight on how parameters such as frame rate and resolution impact scalability.},
	urldate = {2014-09-23},
	booktitle = {Proceeding of the 11th Annual International Conference on Mobile Systems, Applications, and Services},
	publisher = {{ACM}},
	author = {Simoens, Pieter and Xiao, Yu and Pillai, Padmanabhan and Chen, Zhuo and Ha, Kiryong and Satyanarayanan, Mahadev},
	year = {2013},
	keywords = {cloud computing, cloudlet, computer vision, denaturing, google glass, mobile computing, smartphone, virtual machines},
	pages = {139--152},
	file = {ACM Full Text PDF:D\:\\Mannheim\\Zotero\\storage\\8QNNCVGH\\Simoens et al. - 2013 - Scalable Crowd-sourcing of Video from Mobile Devic.pdf:application/pdf}
}

@book{schoeffmann_advances_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture Notes in Computer Science},
	title = {Advances in Multimedia Modeling},
	volume = {7131},
	isbn = {978-3-642-27354-4, 978-3-642-27355-1},
	url = {http://link.springer.com/10.1007/978-3-642-27355-1},
	urldate = {2014-09-23},
	publisher = {Springer Berlin Heidelberg},
	editor = {Schoeffmann, Klaus and Merialdo, Bernard and Hauptmann, Alexander G. and Ngo, Chong-Wah and Andreopoulos, Yiannis and Breiteneder, Christian and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	year = {2012},
	file = {bok%3A978-3-642-27355-1.pdf:D\:\\Mannheim\\Zotero\\storage\\5MN55EKX\\bok%3A978-3-642-27355-1.pdf:application/pdf}
}

@article{cricri_sport_2014,
	title = {Sport Type Classification of Mobile Videos},
	volume = {16},
	issn = {1520-9210},
	doi = {10.1109/TMM.2014.2307552},
	abstract = {The recent proliferation of mobile video content has emphasized the need for applications such as automatic organization and automatic editing of videos. These applications could greatly benefit from domain knowledge about the content. However, extracting semantic information from mobile videos is a challenging task, due to their unconstrained nature. We extract domain knowledge about sport events recorded by multiple users, by classifying the sport type into soccer, American football, basketball, tennis, ice-hockey, or volleyball. We adopt a multi-user and multimodal approach, where each user simultaneously captures audio-visual content and auxiliary sensor data (from magnetometers and accelerometers). Firstly, each modality is separately analyzed; then, analysis results are fused for obtaining the sport type. The auxiliary sensor data is used for extracting more discriminative spatio-temporal visual features and efficient camera motion features. The contribution of each modality to the fusion process is adapted according to the quality of the input data. We performed extensive experiments on data collected at public sport events, showing the merits of using different combinations of modalities and fusion methods. The results indicate that analyzing multimodal and multi-user data, coupled with adaptive fusion, improves classification accuracies in most tested cases, up to 95.45\%.},
	number = {4},
	journal = {{IEEE} Transactions on Multimedia},
	author = {Cricri, F. and Roininen, M.J. and Leppänen, J. and Mate, S. and Curcio, ID.D. and Uhlmann, S. and Gabbouj, M.},
	month = jun,
	year = {2014},
	keywords = {Accelerometers, adaptive fusion, American football, audiovisual content, automatic video editing, automatic video organization, auxiliary sensor data, basketball, camera motion features, Cameras, Data mining, discriminative spatio-temporal visual feature extraction, feature extraction, Fusion, fusion process, ice hockey, image classification, Magnetometers, mobile, mobile computing, mobile video content, modality, multimodal data, multiuser data, public sport events, soccer, sport, sport type classification, tennis, video, Videos, video signal processing, Visualization, volleyball},
	pages = {917--932},
	file = {IEEE Xplore Abstract Record:D\:\\Mannheim\\Zotero\\storage\\BGQJ8KG2\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Mannheim\\Zotero\\storage\\HQBI3HWU\\Cricri et al. - 2014 - Sport Type Classification of Mobile Videos.pdf:application/pdf}
}

@inproceedings{bao_movi:_2010,
	address = {New York, {NY}, {USA}},
	series = {{MobiSys} '10},
	title = {{MoVi}: Mobile Phone Based Video Highlights via Collaborative Sensing},
	isbn = {978-1-60558-985-5},
	shorttitle = {{MoVi}},
	url = {http://doi.acm.org/10.1145/1814433.1814468},
	doi = {10.1145/1814433.1814468},
	abstract = {Sensor networks have been conventionally defined as a network of sensor motes that collaboratively detect events and report them to a remote monitoring station. This paper makes an attempt to extend this notion to the social context by using mobile phones as replacement of motes. We envision a social application where mobile phones collaboratively sense their ambience and recognize socially "interesting" events. The phone with a good view of the event triggers a video recording, and later, the video-clips from different phones are "stitched" into a video highlights of the occasion. We observe that such a video highlights is akin to the notion of event coverage in conventional sensor networks, only the notion of "event" has changed from physical to social. We have built a Mobile Phone based Video Highlights system ({MoVi}) using Nokia phones and {iPod} Nanos, and have experimented in real-life social gatherings. Results show that {MoVi}-generated video highlights (created offline) are quite similar to those created manually, (i.e., by painstakingly editing the entire video of the occasion). In that sense, {MoVi} can be viewed as a collaborative information distillation tool capable of filtering events of social relevance.},
	urldate = {2014-09-23},
	booktitle = {Proceedings of the 8th International Conference on Mobile Systems, Applications, and Services},
	publisher = {{ACM}},
	author = {Bao, Xuan and Roy Choudhury, Romit},
	year = {2010},
	keywords = {collaborative sensing, context, fingerprinting, mobile phones, video highlights},
	pages = {357--370},
	file = {ACM Full Text PDF:D\:\\Mannheim\\Zotero\\storage\\UIU4S4BC\\Bao и Roy Choudhury - 2010 - MoVi Mobile Phone Based Video Highlights via Coll.pdf:application/pdf;MoVi.pdf:D\:\\Mannheim\\Zotero\\storage\\WC58UI9A\\MoVi.pdf:application/pdf}
}

@inproceedings{jain_focus:_2013,
	address = {New York, {NY}, {USA}},
	series = {{SenSys} '13},
	title = {{FOCUS}: Clustering Crowdsourced Videos by Line-of-sight},
	isbn = {978-1-4503-2027-6},
	shorttitle = {{FOCUS}},
	url = {http://doi.acm.org/10.1145/2517351.2517356},
	doi = {10.1145/2517351.2517356},
	abstract = {Crowdsourced video often provides engaging and diverse perspectives not captured by professional videographers. Broad appeal of user-uploaded video has been widely confirmed: freely distributed on {YouTube}, by subscription on Vimeo, and to peers on Facebook/Google+. Unfortunately, user-generated multimedia can be difficult to organize; these services depend on manual "tagging" or machine-mineable viewer comments. While manual indexing can be effective for popular, well-established videos, newer content may be poorly searchable; live video need not apply. We envisage video-sharing services for live user video streams, indexed automatically and in realtime, especially by shared content. We propose {FOCUS}, for Hadoop-on-cloud video-analytics. {FOCUS} uniquely leverages visual, 3D model reconstruction and multimodal sensing to decipher and continuously track a video's line-of-sight. Through spatial reasoning on the relative geometry of multiple video streams, {FOCUS} recognizes shared content even when viewed from diverse angles and distances. In a 70-volunteer user study, {FOCUS}' clustering correctness is roughly comparable to humans.},
	urldate = {2014-09-23},
	booktitle = {Proceedings of the 11th {ACM} Conference on Embedded Networked Sensor Systems},
	publisher = {{ACM}},
	author = {Jain, Puneet and Manweiler, Justin and Acharya, Arup and Beaty, Kirk},
	year = {2013},
	keywords = {crowdsourcing, line-of-sight, live video, multi-view stereo},
	pages = {8:1--8:14},
	file = {ACM Full Text PDF:D\:\\Mannheim\\Zotero\\storage\\GUK9EFEU\\Jain et al. - 2013 - FOCUS Clustering Crowdsourced Videos by Line-of-s.pdf:application/pdf;focus-sensys.pdf:D\:\\Mannheim\\Zotero\\storage\\S3FTTGEG\\focus-sensys.pdf:application/pdf}
}

@inproceedings{Erman:2013,
 author = {Erman, Jeffrey and Ramakrishnan, K.K.},
 title = {Understanding the Super-sized Traffic of the Super Bowl},
 booktitle = {Proceedings of the 2013 Conference on Internet Measurement Conference},
 series = {IMC '13},
 year = {2013},
 isbn = {978-1-4503-1953-9},
 location = {Barcelona, Spain},
 pages = {353--360},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2504730.2504770},
 doi = {10.1145/2504730.2504770},
 acmid = {2504770},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cellular networks, characterization, large venues},
}

@misc{jesse_wilson_androids,
	title = {Android’s {HTTP} Clients {\textbar} Android Developers Blog},
	url = {http://android-developers.blogspot.de/2011/09/androids-http-clients.html},
	urldate = {2014-10-21},
	author = {{Jesse Wilson}},
}


@misc{so_java_cookies,
	title = {java - How do I persist cookies when using {HTTPUrlConnection}? - Stack Overflow},
	shorttitle = {java - How do I persist cookies when using {HTTPUrlConnection}?},
	url = {http://stackoverflow.com/questions/12349266/how-do-i-persist-cookies-when-using-httpurlconnection},
	urldate = {2014-10-21},
}


@misc{nginx,
	title = {Nginx},
	shorttitle = {Nginx},
	url = {http://nginx.org/en/},
	urldate = {2014-10-21},
}

@misc{nginx-rtmp-module,
	title = {arut/nginx-rtmp-module},
	url = {https://github.com/arut/nginx-rtmp-module},
	abstract = {nginx-rtmp-module - {NGINX}-based Media Streaming Server},
	urldate = {2014-10-22},
	journal = {{GitHub}},
}


@misc{apple-hls,
	title = {HTTP Live Streaming - Apple Developer},
	url = {https://developer.apple.com/streaming/},
}


@misc{video-js,
	title = {HTML5 Video Player | Video.js},
	url = {http://www.videojs.com/},
}

@article{duggan_photo_2013,
	title = {Photo and video sharing grow online},
	url = {http://pewinternet.org/~/media/Files/Reports/2013/PIP_Photos%20and%20videos%20online_102813.pdf},
	urldate = {2014-10-22},
	journal = {Pew Internet},
	author = {Duggan, Maeve},
	year = {2013},
}
